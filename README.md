# AI-Ready Agentic Data Platform

End-to-end, open-source AI-ready data platform with real-time streaming, lakehouse modeling, ML pipelines, and an LLM-based analytics agent.

> **New here?** Check out the [Getting Started Guide](GETTING_STARTED.md) for a beginner-friendly, baby-step walkthrough to run this project from scratch.

---

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Event Generator   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Kafka  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Spark Structured Streaming       ‚îÇ
‚îÇ (simulator/)      ‚îÇ     ‚îÇ (Docker)‚îÇ     ‚îÇ                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ  Bronze ‚îÄ‚îÄ‚ñ∂ Silver ‚îÄ‚îÄ‚ñ∂ Gold      ‚îÇ
                                          ‚îÇ  (raw)     (clean)   (business)  ‚îÇ
                                          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                 ‚îÇ       ‚îÇ       ‚îÇ
                                          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                          ‚îÇ       Delta Lake (local FS)      ‚îÇ
                                          ‚îÇ  /data/bronze  /silver  /gold    ‚îÇ
                                          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                 ‚îÇ               ‚îÇ
                                          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                          ‚îÇ  Feature     ‚îÇ ‚îÇ  AI Agent       ‚îÇ
                                          ‚îÇ  Engineering ‚îÇ ‚îÇ  (Ollama LLM)   ‚îÇ
                                          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ  + FAISS search  ‚îÇ
                                                 ‚îÇ        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                          ‚îÇ  ML Model    ‚îÇ
                                          ‚îÇ  (sklearn)   ‚îÇ
                                          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Components

| Component | Location | Purpose |
|---|---|---|
| **Event Simulator** | `simulator/event_generator.py` | Generates random e-commerce events (view, cart, purchase) and streams them to Kafka |
| **Kafka + Zookeeper** | `docker-compose.yml` | Message broker for real-time event ingestion |
| **Bronze Layer** | `spark/bronze.py` | Spark Structured Streaming from Kafka ‚Üí raw Delta Lake (no transformation) |
| **Silver Layer** | `spark/silver.py` | Reads Bronze, validates quality (fails on bad data), cleans, deduplicates |
| **Gold Layer** | `spark/gold.py` | Reads Silver, computes revenue/hour, active users/hour, conversion rate |
| **Feature Engineering** | `features/build_features.py` | PySpark pipeline creating per-user ML features |
| **ML Pipeline** | `ml/train_model.py` | Logistic regression model predicting purchase behavior |
| **AI Agent** | `agent/agent.py` | LLM-powered (Ollama + Mistral) natural language interface over Gold tables with FAISS semantic search |
| **Orchestration** | `airflow/pipeline_dag.py` | Airflow DAGs scheduling Silver ‚Üí Gold ‚Üí Features ‚Üí ML training |

---

## üìÇ Project Structure

```
agentic_data_platform/
‚îú‚îÄ‚îÄ simulator/
‚îÇ   ‚îî‚îÄ‚îÄ event_generator.py       # Kafka event producer
‚îú‚îÄ‚îÄ spark/
‚îÇ   ‚îú‚îÄ‚îÄ bronze.py                # Raw ingestion (Kafka ‚Üí Delta)
‚îÇ   ‚îú‚îÄ‚îÄ silver.py                # Data cleaning & validation
‚îÇ   ‚îî‚îÄ‚îÄ gold.py                  # Business aggregations
‚îú‚îÄ‚îÄ features/
‚îÇ   ‚îî‚îÄ‚îÄ build_features.py        # ML feature engineering
‚îú‚îÄ‚îÄ ml/
‚îÇ   ‚îî‚îÄ‚îÄ train_model.py           # Model training & evaluation
‚îú‚îÄ‚îÄ agent/
‚îÇ   ‚îî‚îÄ‚îÄ agent.py                 # LLM agent with FAISS + Ollama
‚îú‚îÄ‚îÄ airflow/
‚îÇ   ‚îî‚îÄ‚îÄ pipeline_dag.py          # Airflow DAG definitions
‚îú‚îÄ‚îÄ data/                        # Delta Lake storage (auto-created)
‚îÇ   ‚îú‚îÄ‚îÄ bronze/
‚îÇ   ‚îú‚îÄ‚îÄ silver/
‚îÇ   ‚îú‚îÄ‚îÄ gold/
‚îÇ   ‚îú‚îÄ‚îÄ features/
‚îÇ   ‚îî‚îÄ‚îÄ models/
‚îú‚îÄ‚îÄ docker-compose.yml           # Kafka + Zookeeper infrastructure
‚îú‚îÄ‚îÄ requirements.txt             # Python dependencies
‚îî‚îÄ‚îÄ README.md
```

---

## üìã Event Schema

```json
{
  "user_id": "int ‚Äî unique user identifier (1-10000)",
  "product_id": "int ‚Äî product identifier (1-5000)",
  "event_type": "string ‚Äî one of: view, cart, purchase",
  "price": "float ‚Äî 0.0 for view/cart, > 0 for purchase",
  "timestamp": "string ‚Äî ISO-8601 format"
}
```

---

## üöÄ Step-by-Step Run Instructions

### Prerequisites

- Python 3.10+
- Docker & Docker Compose
- Java 11+ (for Spark)
- Ollama installed locally (`curl -fsSL https://ollama.com/install.sh | sh`)

### 1. Install Python Dependencies

```bash
cd agentic_data_platform
pip install -r requirements.txt
```

### 2. Start Kafka & Zookeeper

```bash
docker-compose up -d
```

Wait for services to be healthy, then create the topic:

```bash
docker exec -it kafka kafka-topics --create \
  --topic ecommerce_events \
  --bootstrap-server localhost:9092 \
  --partitions 3 \
  --replication-factor 1
```

Verify the topic:

```bash
docker exec -it kafka kafka-topics --list --bootstrap-server localhost:9092
```

### 3. Start Event Generator

```bash
python simulator/event_generator.py
```

This produces ~1 event/second to the `ecommerce_events` Kafka topic.

### 4. Run Bronze Layer (Streaming Ingestion)

```bash
spark-submit \
  --packages io.delta:delta-spark_2.12:3.1.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 \
  spark/bronze.py
```

### 5. Run Silver Layer (Cleaning & Validation)

```bash
spark-submit --packages io.delta:delta-spark_2.12:3.1.0 spark/silver.py
```

### 6. Run Gold Layer (Business Aggregations)

```bash
spark-submit --packages io.delta:delta-spark_2.12:3.1.0 spark/gold.py
```

### 7. Run Feature Engineering

```bash
spark-submit --packages io.delta:delta-spark_2.12:3.1.0 features/build_features.py
```

### 8. Train ML Model

```bash
python ml/train_model.py
```

### 9. Start AI Agent

```bash
ollama pull mistral
ollama serve   # In a separate terminal
python agent/agent.py
```

**Example interaction:**

```
ü§ñ Ask a question: What is the total revenue?
üìä Answer: Based on the revenue_per_hour table, the total revenue is $X across Y hours...

ü§ñ Ask a question: sql SELECT * FROM revenue_per_hour LIMIT 5
üìä SQL Result:
   hour_window  total_revenue  purchase_count  avg_order_value
0  2025-01-01          150.5              12            12.54
...
```

---

## üìä Data Flow

1. **Event Generator** ‚Üí Produces JSON events to Kafka topic `ecommerce_events` at 1 event/second
2. **Bronze Layer** ‚Üí Spark reads Kafka stream, writes raw data to `data/bronze/ecommerce_events` as Delta Lake (no transformation, checkpointed)
3. **Silver Layer** ‚Üí Reads Bronze Delta table, applies validation & cleaning:
   - Removes null `user_id` and `product_id`
   - Validates `event_type` ‚àà {view, cart, purchase}
   - Removes duplicate events
   - Converts ISO-8601 timestamps
   - **Fails the job** if invalid data is detected (strict quality enforcement)
4. **Gold Layer** ‚Üí Reads Silver, computes business metrics:
   - `revenue_per_hour` ‚Äî Total revenue, purchase count, avg order value per hour window
   - `active_users_per_hour` ‚Äî Distinct active users per hour window
   - `conversion_rate` ‚Äî View‚Üícart and cart‚Üípurchase conversion rates
5. **Feature Engineering** ‚Üí Reads Silver, creates per-user ML features:
   - Purchase statistics (last 24h count, total count, avg order value)
   - Revenue metrics (total, avg, max, min per user)
   - Event frequency (events/hour, total activity span)
   - Conversion ratios (view‚Üícart rate, cart‚Üípurchase rate)
6. **ML Pipeline** ‚Üí Loads user features, trains a logistic regression model to predict purchasers, saves model + metrics
7. **AI Agent** ‚Üí Loads Gold + feature tables into pandas, builds a FAISS vector store for semantic search, and uses Ollama (Mistral) to answer natural language questions via context-rich prompts

---

## ü§ñ How the AI Agent Uses Data

The agent (`agent/agent.py`) provides an interactive natural language interface:

1. **Data Loading** ‚Äî On startup, loads all Gold layer Delta tables and user features as pandas DataFrames
2. **FAISS Indexing** ‚Äî Builds a FAISS vector store from table schemas, summary statistics, and sample data using Ollama embeddings for semantic similarity search
3. **Question Answering** ‚Äî When a user asks a question:
   - Uses FAISS to retrieve the most relevant table context (if available)
   - Falls back to loading all data context if FAISS is unavailable
   - Builds a prompt with table schemas + data context + the user's question
   - Sends the prompt to Ollama (Mistral model) for natural language response
4. **Direct SQL** ‚Äî Users can prefix queries with `sql ` to run SELECT/WHERE/ORDER BY/LIMIT queries directly against loaded DataFrames
5. **Graceful Fallback** ‚Äî If Ollama is not running, returns raw data context so users can still explore their data

### Example Queries

- `"What is the total revenue?"` ‚Äî Summarizes revenue_per_hour data
- `"Why did revenue drop in the last hour?"` ‚Äî Analyzes trends in revenue data
- `"Which users are inactive?"` ‚Äî Examines active_users_per_hour and feature data
- `"Show me the conversion rates"` ‚Äî Returns conversion funnel metrics
- `sql SELECT * FROM revenue_per_hour ORDER BY total_revenue DESC LIMIT 5` ‚Äî Direct data query

---

## ‚ö†Ô∏è Failure Handling

| Component | Failure Strategy |
|---|---|
| **Bronze Layer** | Kafka checkpointing ensures exactly-once semantics; `failOnDataLoss=false` provides resilience against topic compaction |
| **Silver Layer** | Strict data quality validation ‚Äî pipeline **fails with `sys.exit(1)`** if null user_ids, invalid event types, null product_ids, negative prices, or zero-price purchases are found |
| **Gold Layer** | Spark's Delta Lake ACID transactions ensure consistent writes; partial failures don't corrupt data |
| **ML Pipeline** | Warns on missing features; handles insufficient data gracefully with informative messages |
| **AI Agent** | Falls back to raw data display if Ollama LLM is unavailable or errors; FAISS search falls back to full data context on failure |
| **Airflow** | Retries configured (2-3 per task); branch logic skips ML training if fewer than 100 samples exist |
| **Event Generator** | Kafka producer uses `acks='all'` for reliability with success/error callbacks for monitoring |

---

## üìà Scaling Discussion

### Current Design (Local / Single Node)
The platform runs entirely on a single machine using local filesystem storage and Docker containers. This is ideal for development, prototyping, and small-scale analytics.

### Horizontal Scaling Strategies

- **Kafka** ‚Äî Add partitions to the `ecommerce_events` topic and deploy multiple broker instances for higher throughput. Consumer groups enable parallel consumption.
- **Spark** ‚Äî The current local-mode Spark can be replaced with a cluster (Standalone, YARN, or Kubernetes) by changing the master URL. Spark's distributed processing scales linearly with added executors.
- **Delta Lake** ‚Äî Supports ACID transactions, time travel, schema evolution, and Z-ordering for query optimization. Can be backed by distributed storage (HDFS, S3) for larger datasets.
- **Airflow** ‚Äî Switch from `SequentialExecutor` to `CeleryExecutor` or `KubernetesExecutor` for distributed task scheduling across multiple workers.
- **ML Pipeline** ‚Äî Replace logistic regression with gradient boosting (XGBoost/LightGBM) or deep learning models. Add MLflow for experiment tracking, model versioning, and A/B testing.
- **AI Agent** ‚Äî Swap Ollama for a hosted LLM API (OpenAI, Anthropic) for production use. Scale FAISS with IVF indices for millions of documents. Add a proper SQL engine (DuckDB, Trino) for complex analytical queries.
- **Storage** ‚Äî Migrate from local filesystem to object storage (MinIO for on-prem, S3 for cloud) to handle terabyte-scale data volumes.

---

## üõ†Ô∏è Tech Stack

| Tool | Version | Purpose |
|---|---|---|
| Python | 3.10+ | Primary language |
| Docker & Docker Compose | Latest | Infrastructure orchestration |
| Apache Kafka | 7.5.0 (Confluent) | Real-time event streaming |
| Apache Spark | 3.5.0+ | Distributed data processing |
| Delta Lake | 3.1.0+ | ACID lakehouse storage |
| Apache Airflow | 2.8.0+ | Workflow orchestration |
| scikit-learn | 1.3.0+ | ML model training |
| Ollama | Latest | Local LLM serving |
| Mistral | Latest | LLM model for agent |
| LangChain | 0.1.0+ | LLM application framework |
| FAISS | 1.7.4+ | Vector similarity search |
