#!/usr/bin/env python3
"""
Feature Engineering Pipeline
Creates ML features from Silver layer data for model training.
Features:
  - Purchases in last 24 hours (per user)
  - Average order value (per user)
  - Event frequency (per user)
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, count, sum as spark_sum, avg, max as spark_max, min as spark_min,
    when, datediff, current_timestamp, lit, round as spark_round,
    countDistinct, expr, to_timestamp, unix_timestamp
)
from pyspark.sql.window import Window
import sys
from datetime import datetime, timedelta

# Configuration
import os
BASE_PATH = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
SILVER_PATH = os.path.join(BASE_PATH, "data/silver/ecommerce_events")
FEATURES_PATH = os.path.join(BASE_PATH, "data/features/user_features")


def create_spark_session() -> SparkSession:
    """
    Create and configure Spark session with Delta Lake support.
    """
    spark = SparkSession.builder \
        .appName("Feature_Engineering") \
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
        .config("spark.jars.packages", "io.delta:delta-spark_2.12:3.1.0") \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("WARN")
    return spark


def read_silver_data(spark: SparkSession):
    """
    Read data from Silver Delta table.
    """
    print(f"Reading from Silver path: {SILVER_PATH}")
    
    try:
        silver_df = spark.read.format("delta").load(SILVER_PATH)
        record_count = silver_df.count()
        print(f"✓ Read {record_count} records from Silver layer")
        return silver_df
    except Exception as e:
        print(f"✗ Failed to read Silver data: {e}")
        sys.exit(1)


def calculate_purchase_features(df, spark: SparkSession):
    """
    Calculate purchase-related features per user.
    - purchases_last_24h: Number of purchases in last 24 hours
    - total_purchases: Total number of purchases
    - total_revenue: Total revenue generated by user
    - avg_order_value: Average order value
    """
    print("\nCalculating purchase features...")
    
    # Get current timestamp for 24h window calculation
    current_ts = current_timestamp()
    
    # Filter for purchase events
    purchases_df = df.filter(col("event_type") == "purchase")
    
    # Calculate purchase features per user
    purchase_features = purchases_df \
        .groupBy("user_id") \
        .agg(
            # Purchases in last 24 hours
            count(
                when(
                    col("event_timestamp") >= current_ts - expr("INTERVAL 24 HOURS"),
                    1
                )
            ).alias("purchases_last_24h"),
            # Total purchases
            count("*").alias("total_purchases"),
            # Total revenue
            spark_round(spark_sum("price"), 2).alias("total_revenue"),
            # Average order value
            spark_round(avg("price"), 2).alias("avg_order_value"),
            # Max purchase value
            spark_round(spark_max("price"), 2).alias("max_purchase_value"),
            # Min purchase value
            spark_round(spark_min("price"), 2).alias("min_purchase_value")
        )
    
    return purchase_features


def calculate_event_frequency_features(df, spark: SparkSession):
    """
    Calculate event frequency features per user.
    - total_events: Total number of all events
    - view_count: Number of view events
    - cart_count: Number of cart events
    - event_frequency_per_hour: Average events per hour
    """
    print("Calculating event frequency features...")
    
    # Calculate event counts per user
    event_features = df \
        .groupBy("user_id") \
        .agg(
            # Total events
            count("*").alias("total_events"),
            # View count
            count(when(col("event_type") == "view", 1)).alias("view_count"),
            # Cart count
            count(when(col("event_type") == "cart", 1)).alias("cart_count"),
            # Purchase count
            count(when(col("event_type") == "purchase", 1)).alias("event_purchase_count"),
            # Unique products interacted
            countDistinct("product_id").alias("unique_products"),
            # First and last activity
            spark_min("event_timestamp").alias("first_activity"),
            spark_max("event_timestamp").alias("last_activity")
        ) \
        .withColumn(
            # Time span in hours
            "activity_span_hours",
            spark_round(
                (unix_timestamp(col("last_activity")) - unix_timestamp(col("first_activity"))) / 3600,
                2
            )
        ) \
        .withColumn(
            # Events per hour (avoid divide by zero)
            "events_per_hour",
            spark_round(
                when(col("activity_span_hours") > 0, 
                     col("total_events") / col("activity_span_hours"))
                .otherwise(col("total_events")),
                2
            )
        )
    
    return event_features


def calculate_conversion_features(df, spark: SparkSession):
    """
    Calculate conversion-related features per user.
    - view_to_cart_ratio: Cart adds / Views
    - cart_to_purchase_ratio: Purchases / Cart adds
    - overall_conversion: Purchases / Views
    """
    print("Calculating conversion features...")
    
    conversion_features = df \
        .groupBy("user_id") \
        .agg(
            count(when(col("event_type") == "view", 1)).alias("views"),
            count(when(col("event_type") == "cart", 1)).alias("carts"),
            count(when(col("event_type") == "purchase", 1)).alias("purchases")
        ) \
        .withColumn(
            "view_to_cart_ratio",
            spark_round(
                when(col("views") > 0, col("carts") / col("views"))
                .otherwise(0.0),
                4
            )
        ) \
        .withColumn(
            "cart_to_purchase_ratio",
            spark_round(
                when(col("carts") > 0, col("purchases") / col("carts"))
                .otherwise(0.0),
                4
            )
        ) \
        .withColumn(
            "overall_conversion",
            spark_round(
                when(col("views") > 0, col("purchases") / col("views"))
                .otherwise(0.0),
                4
            )
        ) \
        .select(
            "user_id",
            "view_to_cart_ratio",
            "cart_to_purchase_ratio",
            "overall_conversion"
        )
    
    return conversion_features


def combine_features(purchase_features, event_features, conversion_features):
    """
    Combine all feature sets into a single feature table.
    """
    print("\nCombining all features...")
    
    # Start with event features (all users)
    combined = event_features \
        .join(purchase_features, on="user_id", how="left") \
        .join(conversion_features, on="user_id", how="left")
    
    # Fill nulls for users without purchases
    combined = combined \
        .fillna(0, subset=[
            "purchases_last_24h", "total_purchases", "total_revenue",
            "avg_order_value", "max_purchase_value", "min_purchase_value"
        ])
    
    # Add feature timestamp
    combined = combined.withColumn("feature_timestamp", current_timestamp())
    
    # Create target variable: is_purchaser (binary classification target)
    combined = combined.withColumn(
        "is_purchaser",
        when(col("total_purchases") > 0, 1).otherwise(0)
    )
    
    # Select final feature columns
    final_features = combined.select(
        "user_id",
        # Purchase features
        "purchases_last_24h",
        "total_purchases",
        "total_revenue",
        "avg_order_value",
        "max_purchase_value",
        "min_purchase_value",
        # Event frequency features
        "total_events",
        "view_count",
        "cart_count",
        "unique_products",
        "activity_span_hours",
        "events_per_hour",
        # Conversion features
        "view_to_cart_ratio",
        "cart_to_purchase_ratio",
        "overall_conversion",
        # Metadata
        "first_activity",
        "last_activity",
        "feature_timestamp",
        # Target
        "is_purchaser"
    )
    
    return final_features


def write_features(df, output_path: str):
    """
    Write feature table to Delta Lake.
    """
    print(f"\nWriting features to: {output_path}")
    
    df.write \
        .format("delta") \
        .mode("overwrite") \
        .option("overwriteSchema", "true") \
        .save(output_path)
    
    record_count = df.count()
    print(f"✓ Wrote {record_count} user feature records")


def main():
    """
    Main function to run the feature engineering pipeline.
    """
    print("=" * 60)
    print("Feature Engineering Pipeline")
    print("=" * 60)
    print(f"Silver Path: {SILVER_PATH}")
    print(f"Features Path: {FEATURES_PATH}")
    print("=" * 60)
    
    # Create Spark session
    spark = create_spark_session()
    print("✓ Spark session created")
    
    # Read Silver data
    silver_df = read_silver_data(spark)
    
    # Cache for multiple passes
    silver_df.cache()
    
    # Calculate feature groups
    purchase_features = calculate_purchase_features(silver_df, spark)
    event_features = calculate_event_frequency_features(silver_df, spark)
    conversion_features = calculate_conversion_features(silver_df, spark)
    
    # Combine all features
    final_features = combine_features(purchase_features, event_features, conversion_features)
    
    # Show sample
    print("\nSample Feature Records:")
    final_features.select(
        "user_id", "total_events", "total_purchases", 
        "avg_order_value", "events_per_hour", "is_purchaser"
    ).show(10)
    
    # Write to Delta
    write_features(final_features, FEATURES_PATH)
    
    # Unpersist
    silver_df.unpersist()
    
    print("\n" + "=" * 60)
    print("Feature Engineering Complete")
    print("=" * 60)
    
    spark.stop()


if __name__ == "__main__":
    main()
